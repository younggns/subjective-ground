{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Downloading (social-chemistry-101 dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "import zipfile\n",
    "\n",
    "\"\"\"\n",
    "Download the social-chemistry-101 zip file and unzip it\n",
    "\"\"\"\n",
    "url = \"https://storage.googleapis.com/ai2-mosaic-public/projects/social-chemistry/data/social-chem-101.zip\"\n",
    "data_dir = os.getcwd()+\"/data/\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "wget.download(url, data_dir)\n",
    "\n",
    "with zipfile.ZipFile(data_dir+'/social-chem-101.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading & filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.getcwd()+'/data/social-chem-101/social-chem-101.v1.0.tsv', sep='\\t')\n",
    "\n",
    "df_AITA = df[df['area']=='amitheasshole']\n",
    "df_AITA = df_AITA[['split', 'situation', 'rot', 'situation-short-id', 'rot-moral-foundations', 'rot-judgment', 'rot-id']]\n",
    "\n",
    "df_AITA.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rules of thumb per situation: {1: 5502, 3: 6672, 4: 2599, 2: 12801, 5: 1193}\n"
     ]
    }
   ],
   "source": [
    "# Review social-chemistry-101 data stats\n",
    "\n",
    "def return_rotCnt_stat(df):\n",
    "    situationID_rotCnt = Counter()\n",
    "    situationID_rotCnt.update(df['situation-short-id'].tolist())\n",
    "\n",
    "    rotCnt_IDs = {}\n",
    "    for key, cnt in situationID_rotCnt.items():\n",
    "        if cnt not in rotCnt_IDs:\n",
    "            rotCnt_IDs[cnt] = [key]\n",
    "        else:\n",
    "            rotCnt_IDs[cnt].append(key)\n",
    "\n",
    "    rotCnt_stat = {}\n",
    "    for cnt in rotCnt_IDs:\n",
    "        rotCnt_stat[cnt] = len(list(set(rotCnt_IDs[cnt])))\n",
    "\n",
    "    return rotCnt_stat\n",
    "\n",
    "print('# rules of thumb per situation:',return_rotCnt_stat(df_AITA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand the number of rule of thumb for situations\n",
    "\n",
    "\n",
    "### Strategy\n",
    "- Change judgments for the same action (1 RoT -> expand it to 5 RoTs)\n",
    "- Remove redundant items (similar judgments for same actions)\n",
    "- Keep K (=5) items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "judgment_group = {\n",
    "    \"shouldn't\": ['should not', \"shouldn't\"],\n",
    "    'should': ['should', \"should\"],\n",
    "    'okay': ['okay', 'ok', 'fine', 'normal', 'acceptable', 'understandable', 'allowed'],\n",
    "    'not-okay': ['not okay','not ok', 'not fine', 'not normal', 'not acceptable', 'not undedrstandable', 'not allowed'],\n",
    "    'nice': ['nice', 'kind', 'polite', 'considerate', 'good'],\n",
    "    'rude': ['hurtful', 'mean', 'rude', 'inconsiderate', 'bad'],\n",
    "    'right': ['right', 'expected', 'reasonable', 'appropriate'],\n",
    "    'wrong': ['wrong', 'not expected', 'unreasonable', 'inappropriate']\n",
    "}\n",
    "mapping_rules = {\n",
    "    'should':[\"shouldn't\", 'okay', 'not-okay', 'nice', 'rude'],\n",
    "    \"shouldn't\": ['should', 'okay', 'not-okay', 'nice', 'rude'],\n",
    "    'okay':['not-okay', 'nice', 'rude', 'right', 'wrong'],\n",
    "    'not-okay': ['okay', 'nice', 'rude', 'right', 'wrong'],\n",
    "    'nice': ['rude', 'okay', 'not-okay', 'right', 'wrong'],\n",
    "    'rude': ['nice', 'okay', 'not-okay', 'right', 'wrong'],\n",
    "    'right': ['wrong', 'okay', 'not-okay', 'nice', 'rude'],\n",
    "    'wrong': ['right', 'okay', 'not-okay', 'nice', 'rude'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_RoT(_judgment, _rot):\n",
    "    # identify key judgment words\n",
    "    _judgment_word, _judgment_group_key = '', ''\n",
    "    word_list = _judgment.split()\n",
    "    \n",
    "    for key in judgment_group:    \n",
    "        for word in word_list:\n",
    "            if word.lower() == 'should' and word_list.index(word) < len(word_list)-1 and \\\n",
    "                                    word_list[word_list.index(word)+1].lower() == 'not':\n",
    "                _judgment_group_key, _judgment_word = \"shouldn't\", 'should not'\n",
    "                break\n",
    "            else:\n",
    "                if word.lower() in judgment_group[key]:\n",
    "                    _judgment_group_key, _judgment_word = key, word.lower()\n",
    "                    break\n",
    "        \n",
    "        if _judgment_word != '':\n",
    "            break\n",
    "\n",
    "    if _judgment_word == '':\n",
    "        return [], []\n",
    "    \n",
    "    mapping_keys = mapping_rules[_judgment_group_key]\n",
    "    generated_rots, generated_judgments = [], []\n",
    "    for mapping_idx, elem in enumerate(mapping_keys):\n",
    "        random_idx = np.random.randint(len(judgment_group[elem]), size=1)[0]\n",
    "        generated_rots.append(_rot.lower().replace(_judgment_word, judgment_group[elem][random_idx]))\n",
    "        generated_judgments.append(_judgment.lower().replace(_judgment_word, judgment_group[elem][random_idx]))\n",
    "        \n",
    "    return generated_rots, generated_judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate RoTs\n",
    "\n",
    "additional_row_series = []\n",
    "for idx, row in df_AITA.iterrows():\n",
    "    generated_rots, generated_judgments = generate_RoT(row['rot-judgment'], row['rot'])\n",
    "    for _new_rot, _new_judgment in zip(generated_rots, generated_judgments):\n",
    "        copy_row = row.copy()\n",
    "        copy_row['rot'] = _new_rot\n",
    "        copy_row['rot-judgment'] = _new_judgment\n",
    "        additional_row_series.append(copy_row)\n",
    "\n",
    "\n",
    "df_additional = pd.DataFrame(additional_row_series)\n",
    "\n",
    "df_all = pd.concat([df_AITA, df_additional])\n",
    "\n",
    "generated = [0 for elem in range(len(df_AITA))]\n",
    "generated += [1 for elem in range(len(df_additional))]\n",
    "\n",
    "df_all['generated'] = generated\n",
    "df_all.sort_values(by=['split', 'situation-short-id'], ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28767/28767 [04:45<00:00, 100.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicated RoTs\n",
    "\n",
    "duplicated_indices = []\n",
    "for _id in tqdm(list(set(df_all['situation-short-id'].tolist()))):\n",
    "    _df = df_all[df_all['situation-short-id'].isin([_id])]\n",
    "    duplicated_indices += _df[_df.duplicated(subset=['rot'])].index.tolist()\n",
    "\n",
    "df_all.drop(duplicated_indices, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(os.getcwd()+'/data/social-chem-101/RoT-augmented.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep K rules-of-thumb per situation\n",
    "\n",
    "- Remove situations where their total # of rules-of-thumb is less than K\n",
    "- If RoTs are not generated, keep all of them\n",
    "- Per situation, get the number of RoT that we should fill in\n",
    "- Compute vector representation of non-generated RoTs\n",
    "- Get the 'farthest distance' items among generated RoTs (so that we get diverse RoTs per situation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1315 situations that have less than 5 rules-of-thumb\n"
     ]
    }
   ],
   "source": [
    "# remove situation ids that have less than 5 counts\n",
    "\n",
    "situationID_rotCnt = Counter()\n",
    "situationID_rotCnt.update(df_all['situation-short-id'].tolist())\n",
    "\n",
    "situationIDs_to_remove = [key for key, value in situationID_rotCnt.items() if value < 5]\n",
    "df_all = df_all[~df_all['situation-short-id'].isin(situationIDs_to_remove)]\n",
    "\n",
    "print('Removed %d situations that have less than %d rules-of-thumb'%(len(situationIDs_to_remove), k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28767/28767 [04:30<00:00, 106.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# Approach 1. Random selection\n",
    "\n",
    "np.random.seed(624)\n",
    "\n",
    "random_items_indices = []\n",
    "for _id in tqdm(situationID_rotCnt.keys()):\n",
    "    _df = df_all[df_all['situation-short-id'].isin([_id])]\n",
    "    _curr_appended_indices = []\n",
    "    \n",
    "    if len(_df) == 0:\n",
    "        continue\n",
    "    \n",
    "    # 0. when # rules-of-thumb is already K\n",
    "    if len(_df) == 5:\n",
    "        _curr_appended_indices += _df.index.tolist()\n",
    "    \n",
    "    else:\n",
    "        # 1. include annotated ROTs\n",
    "        _curr_appended_indices += _df[_df['generated']==0].index.tolist()\n",
    "\n",
    "        # 2. Random selection\n",
    "        RoT_candidate_indices = _df[~_df.index.isin(_curr_appended_indices)].index.tolist()\n",
    "        remaining_items = k - len(_curr_appended_indices)\n",
    "        _curr_appended_indices += list(np.random.choice(RoT_candidate_indices, size=remaining_items, replace=False))\n",
    "\n",
    "    random_items_indices += _curr_appended_indices\n",
    "\n",
    "df_topk_random = df_all[df_all.index.isin(random_items_indices)]\n",
    "df_topk_random.sort_values(by=['split', 'situation-short-id'], inplace=True, ignore_index=True)\n",
    "df_topk_random.to_csv(os.getcwd()+'/../data/social-chem-101/RoT-augmented-topK-random.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28767/28767 [04:47<00:00, 100.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# Approach 2. Balanced selection\n",
    "\n",
    "np.random.seed(624)\n",
    "\n",
    "balanced_items_indices = []\n",
    "for _id in tqdm(situationID_rotCnt.keys()):\n",
    "    _df = df_all[df_all['situation-short-id'].isin([_id])]\n",
    "    _curr_appended_indices = []\n",
    "    \n",
    "    if len(_df) == 0:\n",
    "        continue\n",
    "    \n",
    "    # 0. when # rules-of-thumb is already K\n",
    "    if len(_df) == 5:\n",
    "        _curr_appended_indices += _df.index.tolist()\n",
    "        balanced_items_indices += _curr_appended_indices\n",
    "        continue\n",
    "\n",
    "    # 1. include annotated ROTs\n",
    "    _curr_appended_indices += _df[_df['generated']==0].index.tolist()\n",
    "    if len(_curr_appended_indices) == k:\n",
    "        balanced_items_indices += _curr_appended_indices\n",
    "        continue\n",
    "        \n",
    "    remaining_items = k - len(_curr_appended_indices)\n",
    "    _df_candidate = _df[~_df.index.isin(_curr_appended_indices)]\n",
    "    remaining_RoT_IDs = list(set(_df_candidate['rot-id'].tolist()))\n",
    "        \n",
    "    if len(remaining_RoT_IDs) >= remaining_items:\n",
    "        _over_appended_indices = []\n",
    "        for _remaining_id in remaining_RoT_IDs:\n",
    "            _df_remaining = _df_candidate[_df_candidate['rot-id'].isin([_remaining_id])]\n",
    "            _df_remaining_indices = _df_remaining.index.tolist()\n",
    "            _over_appended_indices += list(np.random.choice(_df_remaining_indices, size=1))\n",
    "        _curr_appended_indices += list(np.random.choice(_over_appended_indices, size=remaining_items, replace=False))\n",
    "    else:\n",
    "        _under_appended_indices = []\n",
    "        for _remaining_id in remaining_RoT_IDs:\n",
    "            _df_remaining = _df_candidate[_df_candidate['rot-id'].isin([_remaining_id])]\n",
    "            _df_remaining_indices = _df_remaining.index.tolist()\n",
    "            _under_appended_indices += list(np.random.choice(_df_remaining_indices, size=1))\n",
    "        \n",
    "        _rest_indices = [elem for elem in _df_candidate.index.tolist() if elem not in _under_appended_indices]\n",
    "        _under_appended_indices += list(np.random.choice(_rest_indices, size=remaining_items - len(_under_appended_indices), replace=False))\n",
    "        _curr_appended_indices += _under_appended_indices\n",
    "        \n",
    "    balanced_items_indices += _curr_appended_indices\n",
    "    \n",
    "    \n",
    "df_topk_balanced = df_all[df_all.index.isin(balanced_items_indices)]\n",
    "df_topk_balanced.sort_values(by=['split', 'situation-short-id'], inplace=True, ignore_index=True)\n",
    "df_topk_balanced.to_csv(os.getcwd()+'/data/social-chem-101/RoT-augmented-topK-balanced.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Approach 3. Distance-based selection\n",
    "# \"\"\"\n",
    "# distance-based selection tends to prefer one RoT (distances are not diverse)\n",
    "# however, we want to include generated RoTs as diverse as possible\n",
    "# -> discard the method\n",
    "# \"\"\"\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from sklearn import metrics\n",
    "\n",
    "# \"\"\"\n",
    "# # Run this in the gpu-enabled machine\n",
    "\n",
    "# smodel = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "# all_RoTs = df_all['rot'].tolist()\n",
    "# all_RoT_embeddings = smodel.encode(all_RoTs)\n",
    "# \"\"\"\n",
    "\n",
    "# all_RoT_embeddings = np.load(open(os.getcwd()+'/data/social-chem-101/RoT_embeddings.npy', 'rb'))\n",
    "\n",
    "# dfIndex_to_npyIdx = {}\n",
    "# for idx, elem in enumerate(df_all.index.tolist()):\n",
    "#     dfIndex_to_npyIdx[elem] = idx\n",
    "    \n",
    "# distance_items_indices = []\n",
    "# for _id in tqdm(situationID_rotCnt.keys()):\n",
    "#     _df = df_all[df_all['situation-short-id'].isin([_id])]\n",
    "#     _curr_appended_indices = []\n",
    "    \n",
    "#     if len(_df) == 0:\n",
    "#         continue\n",
    "        \n",
    "#     # 0. when # rules-of-thumb is already K\n",
    "#     if len(_df) == 5:\n",
    "#         _curr_appended_indices += _df.index.tolist()\n",
    "#         distance_items_indices += _curr_appended_indices\n",
    "    \n",
    "#     _curr_appended_indices += _df[_df['generated']==0].index.tolist()\n",
    "#     if len(_curr_appended_indices) == k:\n",
    "#         distance_items_indices += _curr_appended_indices\n",
    "#         continue\n",
    "\n",
    "\n",
    "#     # 2. Random selection\n",
    "#     remaining_items = k - len(_curr_appended_indices)\n",
    "\n",
    "#     annot_indices = [dfIndex_to_npyIdx[elem] for elem in _df[_df['generated']==0].index.tolist()]\n",
    "#     annot_embeddings = all_RoT_embeddings[annot_indices]\n",
    "\n",
    "#     RoT_candidate_indices = _df[~_df.index.isin(_curr_appended_indices)].index.tolist()\n",
    "#     RoT_candidate_embIdx = [dfIndex_to_npyIdx[elem] for elem in RoT_candidate_indices]\n",
    "#     RoT_candidate_embeddings = all_RoT_embeddings[RoT_candidate_embIdx]\n",
    "\n",
    "#     _dists = metrics.pairwise_distances(annot_embeddings, RoT_candidate_embeddings) # shape: (len_annot X len_candidates)\n",
    "#     _dists_sorted = sorted(enumerate(list(np.mean(_dists, axis=0))), key=lambda x:x[1], reverse=True)\n",
    "#     _farthest_indices = [elem[0] for elem in _dists_sorted[:remaining_items]]\n",
    "\n",
    "#     _curr_appended_indices += [RoT_candidate_indices[elem] for elem in _farthest_indices]\n",
    "\n",
    "#     distance_items_indices += _curr_appended_indices\n",
    "\n",
    "# df_topk_distance = df_all[df_all.index.isin(distance_items_indices)]\n",
    "# df_topk_distance.sort_values(by=['split', 'situation-short-id'], inplace=True, ignore_index=True)\n",
    "# df_topk_distance.to_csv(os.getcwd()+'/data/social-chem-101/RoT-augmented-topK-distance.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: all rules-of-thumb per situation should be length of k ( = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topK-random: # rules of thumb per situation: {5: 27452}\n",
      "topK-balanced: # rules of thumb per situation: {5: 27452}\n"
     ]
    }
   ],
   "source": [
    "print('topK-random: # rules of thumb per situation:',return_rotCnt_stat(df_topk_random))\n",
    "print('topK-balanced: # rules of thumb per situation:',return_rotCnt_stat(df_topk_balanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate with most active users\n",
    "\n",
    "- By running */data_crawling/get_AITAsocchem_comms.py*, we get all comments left on social-chem-101 situations -> */data/reddit-morality/Comms__(Sub_Sochem)__(Redditor_all).tsv*\n",
    "- By running */data_crawling/get_redditor_comms.py*, we first identify 30 most active redditors who commented on r/AmITheAsshole in the social-chem-101 dataset. Then we get all other comments of the top redditors -> */data/reddit-morality/Comms__(Sub_AITA)__(Redditor_active).tsv*\n",
    "    - This file doesn't have info about the reddit post titles, which we will use later in clustering comments\n",
    "    - Use */data_crawling/get_titles.py* to align comments with submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yh/opt/anaconda3/envs/dev37/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3417: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# Loading additional dataframes\n",
    "\n",
    "df_Comms_Sochem_all = pd.read_csv(os.getcwd()+'/data/reddit-morality/Comms__(Sub_Sochem)__(Redditor_all).tsv', sep='\\t')\n",
    "df_Comms_AITA_active = pd.read_csv(os.getcwd()+'/data/reddit-morality/Comms__(Sub_AITA)__(Redditor_active).tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain 'id' column to compare different dataframes\n",
    "\n",
    "df_topk_balanced['id'] = df_topk_balanced['situation-short-id'].map(lambda x:x.split('reddit/amitheasshole/')[1])\n",
    "df_topk_balanced = df_topk_balanced[['split', 'situation', 'rot', 'id', 'rot-moral-foundations']]\n",
    "\n",
    "\n",
    "df_Comms_AITA_active.dropna(inplace=True)\n",
    "df_Comms_AITA_active['id'] = df_Comms_AITA_active['permalink'].map(lambda x:str(x).split('/r/AmItheAsshole/comments/')[1].split('/')[0])\n",
    "df_Comms_AITA_active = df_Comms_AITA_active[['author', 'body', 'id', 'permalink']]\n",
    "\n",
    "\n",
    "df_Comms_Sochem_all.dropna(inplace=True)\n",
    "df_Comms_Sochem_all['id'] = df_Comms_Sochem_all['permalink'].map(lambda x:str(x).split('/r/AmItheAsshole/comments/')[1].split('/')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold: 530, # active redditors: 12\n",
      "looking at 6547 situations out of 27452 (0.2385)\n",
      "train: 4961, valid: 597, test: 567\n",
      "least active redditor history in AITA: 1299\n",
      "num submission IDs to crawl: 35442\n",
      "\n",
      "threshold: 354, # active redditors: 30\n",
      "looking at 10811 situations out of 27452 (0.3938)\n",
      "train: 8102, valid: 991, test: 1000\n",
      "least active redditor history in AITA: 928\n",
      "num submission IDs to crawl: 59919\n",
      "\n",
      "threshold: 150, # active redditors: 209\n",
      "looking at 21198 situations out of 27452 (0.7722)\n",
      "train: 15756, valid: 1943, test: 2019\n",
      "least active redditor history in AITA: 319\n",
      "num submission IDs to crawl: 229320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def finding_num_active_redditors(thresholds):\n",
    "    author_cnt = Counter()\n",
    "    author_cnt.update(df_Comms_Sochem_all.author.tolist())\n",
    "    author_cnt_sorted = sorted(author_cnt.items(), key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    for _thr in thresholds:\n",
    "        most_active_redditors = [elem[0] for elem in author_cnt_sorted if elem[1] > _thr and elem[0] != 'AutoModerator']\n",
    "        minidf = df_Comms_Sochem_all[df_Comms_Sochem_all['author'].isin(most_active_redditors)]\n",
    "        new_ids = list(set(minidf['id'].tolist()))\n",
    "        \n",
    "        _split_cnt = Counter()\n",
    "        _split_cnt.update(df_topk_balanced_agg[df_topk_balanced_agg['id'].isin(new_ids)]['split'].tolist())\n",
    "        \n",
    "        \n",
    "        _cnt = Counter()\n",
    "        _cnt.update(df_Comms_AITA_active[df_Comms_AITA_active['author'].isin(most_active_redditors)].author.tolist())\n",
    "        _cnt_sorted = sorted(_cnt.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "        print('threshold: %d, # active redditors: %d'%(_thr, len(most_active_redditors)))\n",
    "        print('looking at %d situations out of %d (%.4f)'%(len(new_ids), len(df_topk_balanced_agg), len(new_ids)/len(df_topk_balanced_agg)))\n",
    "        print('train: %d, valid: %d, test: %d'%(_split_cnt['train'], _split_cnt['dev'], _split_cnt['test']))\n",
    "        print('least active redditor history in AITA: %d'%_cnt_sorted[-1][1])\n",
    "        print('num submission IDs to crawl: %d'%len(list(set(df_Comms_AITA_active[df_Comms_AITA_active['author'].isin(most_active_redditors)]['id'].tolist()))))\n",
    "        print()\n",
    "        \n",
    "finding_num_active_redditors([530, 354, 150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idnetify 30 most active redditors\n",
    "\n",
    "author_cnt = Counter()\n",
    "author_cnt.update(df_Comms_Sochem_all.author.tolist())\n",
    "author_cnt_sorted = sorted(author_cnt.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "most_active_redditors = [elem[0] for elem in author_cnt_sorted if elem[1] > 354 and elem[0] != 'AutoModerator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check top redditors' prev history stats on AITA\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "author_all_cnt = Counter()\n",
    "author_all_cnt.update(df_Comms_AITA_active[df_Comms_AITA_active['author'].isin(most_active_redditors)].author.tolist())\n",
    "author_all_cnt_sorted = sorted(author_all_cnt.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "# print(author_all_cnt_sorted)\n",
    "\n",
    "plt.bar(range(len(author_all_cnt_sorted)), [elem[1] for elem in author_all_cnt_sorted])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AITA submission IDs from the most active redditors\n",
    "\n",
    "most_active_redditor_submissionIDs = list(set(df_Comms_AITA_active[df_Comms_AITA_active['author'].isin(most_active_redditors)]['id'].tolist()))\n",
    "\n",
    "\"\"\"\n",
    "Use this list to get reddit post title & selftext using praw API (praw_Submissions_by_submissionID.py)\n",
    "This is necessary since we will be clustering redditor comments based on the reddit post titles, \n",
    " so that comments are clustered based on the topic of reddit posts\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_keep_order(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate RoTs\n",
    "\n",
    "agg_splits = remove_duplicates_keep_order(df_topk_balanced['split'].tolist())\n",
    "agg_situations = remove_duplicates_keep_order(df_topk_balanced['situation'].tolist())\n",
    "agg_ids = remove_duplicates_keep_order(df_topk_balanced['id'].tolist())\n",
    "agg_RoTs = df_topk_balanced.groupby('id', sort=False)['rot'].agg(lambda x:'_____'.join(x)).tolist()\n",
    "agg_MFs = df_topk_balanced.groupby('id', sort=False)['rot-moral-foundations'].agg(lambda x:'_____'.join(x)).tolist()\n",
    "\n",
    "assert len(agg_splits) == len(agg_RoTs) == len(agg_MFs)\n",
    "\n",
    "df_topk_balanced_agg = pd.DataFrame(data={'split': agg_splits, 'situation': agg_situations, \n",
    "                        'rot': agg_RoTs, 'id': agg_ids, 'rot-moral-foundations': agg_MFs})\n",
    "\n",
    "df_topk_balanced_agg.to_csv(os.getcwd()+'/data/social-chem-101/RoT-augmented-topK-balanced-aggregated.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 36.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# Aggregate Comments\n",
    "\n",
    "data_dict = {'author':[], 'comment':[], 'id':[]}\n",
    "df_Comms_AITA_top = df_Comms_AITA_active[df_Comms_AITA_active['author'].isin(most_active_redditors)]\n",
    "\n",
    "for _author in most_active_redditors:\n",
    "    minidf = df_Comms_AITA_top[df_Comms_AITA_top['author']==_author]\n",
    "    \n",
    "    agg_comments = minidf.groupby('id', sort=False)['body'].agg(lambda x:'_____'.join(x)).tolist()\n",
    "    agg_ids = remove_duplicates_keep_order(minidf['id'].tolist())\n",
    "    \n",
    "    assert len(agg_comments) == len(agg_ids)\n",
    "    \n",
    "    data_dict['id'] += agg_ids\n",
    "    data_dict['comment'] += agg_comments\n",
    "    data_dict['author'] += [_author]*len(agg_comments)\n",
    "\n",
    "df_Comms_AITA_top_agg = pd.DataFrame(data=data_dict)\n",
    "df_Comms_AITA_top_agg.to_csv(os.getcwd()+'/../data/reddit-morality/Comms__(Sub_AITA)__(Redditor_top)__agg.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59919/59919 [03:55<00:00, 254.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Match submission IDs and merge the two dataframes\n",
    "\n",
    "\"\"\"\n",
    "- For all submission IDs found in df_Comms_AITA_top_agg\n",
    "    - Get its split, situation, RoTs, MFs from df_topk_balanced_agg\n",
    "    - Identify authors who participated\n",
    "        - Get their aggregated comments\n",
    "\"\"\"\n",
    "\n",
    "data_dict = {'split':[], 'situation':[], 'comment':[], 'author':[], 'rot':[], 'rot-moral-foundations':[], 'id':[]}\n",
    "for _id in tqdm(list(set(df_Comms_AITA_top_agg['id']))):\n",
    "    \n",
    "    _df_match_from_topk = df_topk_balanced_agg[df_topk_balanced_agg['id']==_id]\n",
    "    if len(_df_match_from_topk) == 0:\n",
    "        continue\n",
    "        \n",
    "    _df_match_from_comms = df_Comms_AITA_top_agg[df_Comms_AITA_top_agg['id'] == _id]\n",
    "    \n",
    "    data_dict['author'] += _df_match_from_comms['author'].tolist()\n",
    "    data_dict['comment'] += _df_match_from_comms['comment'].tolist()\n",
    "    \n",
    "    num_participated_redditors = len(_df_match_from_comms)\n",
    "    \n",
    "    data_dict['split'] += _df_match_from_topk['split'].tolist() * num_participated_redditors\n",
    "    data_dict['situation'] += _df_match_from_topk['situation'].tolist() * num_participated_redditors\n",
    "    data_dict['rot'] += _df_match_from_topk['rot'].tolist() * num_participated_redditors\n",
    "    data_dict['rot-moral-foundations'] += _df_match_from_topk['rot-moral-foundations'].tolist() * num_participated_redditors\n",
    "    \n",
    "    data_dict['id'] += [_id] * num_participated_redditors\n",
    "    \n",
    "    assert len(data_dict['author']) == len(data_dict['split']) == len(data_dict['id'])\n",
    "\n",
    "df_topk_Comms = pd.DataFrame(data=data_dict)\n",
    "df_topk_Comms.sort_values(by=['split', 'id'], ascending=False, inplace=True)\n",
    "df_topk_Comms.to_csv(os.getcwd()+'/data/reddit-morality/CommsRoTs__(Sub_Sochem)__(Redditor_top)__(Judg_all).tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncate comments\n",
    "\n",
    "Now we truncate comments based on whether they include proper AITA judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AITA_codebook = {'YTA':'YTA', 'NTA':'NTA', 'ESH':'ESH', 'NAH':'NAH', 'INFO':'INFO',\n",
    "                'YWBTA':'YTA', 'YWNBTA':'NTA'}\n",
    "AITA_codebook_lower = {'yta':'YTA', 'nta':'NTA', 'esh':'ESH', 'nah':'NAH', 'info':'INFO',\n",
    "                       'Yta':'YTA', 'Nta':'NTA', 'Esh': 'ESH', 'Nah':'NAH', 'Info':'INFO',\n",
    "                       'Ywbta':'YTA', 'Ywnbta':'NTA'}\n",
    "AITA_codebook_expand = {\n",
    "    \"you're the asshole\":'YTA', 'you are the asshole':'YTA', 'you are an asshole':'YTA', \"you're an asshole\":'YTA',\n",
    "    \"you're the a-hole\":'YTA', 'you are the a-hole':'YTA', 'you are an a-hole':'YTA', \"you're an a-hole\":'YTA',\n",
    "    \"you're the ahole\":'YTA', 'you are the ahole':'YTA', 'you are an ahole':'YTA', \"you're an ahole\":'YTA',\n",
    "    'you would be the asshole':'YTA', \"you'd be the asshole\":'YTA', 'you would be an asshole':'YTA', \"you'd be an asshole\":'YTA',\n",
    "    'you would be the a-hole':'YTA', \"you'd be the a-hole\":'YTA', 'you would be an a-hole':'YTA', \"you'd be an a-hole\":'YTA',\n",
    "    'you would be the ahole':'YTA', \"you'd be the ahole\":'YTA', 'you would be an ahole':'YTA', \"you'd be an ahole\":'YTA',\n",
    "    \n",
    "    \"not the a-hole\":'NTA', \"not the ahole\":'NTA', \"not the asshole\":'NTA', \n",
    "    \"not an a-hole\":'NTA', \"not an ahole\":'NTA', \"not an asshole\":'NTA', \n",
    "    \"not be the a-hole\":'NTA', \"not be the ahole\":'NTA', \"not be the asshole\":'NTA', \n",
    "    \"not an the a-hole\":'NTA', \"not an the ahole\":'NTA', \"not an the asshole\":'NTA', \n",
    "    \n",
    "    \"everyone sucks here\":'ESH', \n",
    "    \n",
    "    'no a-holes here':'NAH', 'no aholes here':'NAH', 'no assholes here':'NAH',\n",
    "    \n",
    "    'not enough info': 'INFO'\n",
    "}\n",
    "    \n",
    "            \n",
    "    \n",
    "def code_match(codebook, text, votes, match):\n",
    "    answer = ''\n",
    "    for _code in codebook.keys():\n",
    "        if _code in text:\n",
    "            votes[codebook[_code]] += 1\n",
    "            match = True\n",
    "            answer = codebook[_code]\n",
    "\n",
    "    return votes, match, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_judgment_remove_commsWithoutJudgment(_df):\n",
    "    subID_author_to_votes = {}\n",
    "    matching_indices, redditor_answers = [], []\n",
    "\n",
    "    for idx, row in _df.iterrows():\n",
    "        _subID_author = row['id'] + '___###___' + row['author']\n",
    "        if _subID_author in subID_author_to_votes:\n",
    "            match = False\n",
    "\n",
    "            subID_author_to_votes[_subID_author], match, answer = code_match(AITA_codebook, \n",
    "                                                row['comment'], subID_author_to_votes[_subID_author], match)\n",
    "            if not match:\n",
    "                subID_author_to_votes[_subID_author], match, answer = code_match(AITA_codebook_lower, \n",
    "                                                row['comment'], subID_author_to_votes[_subID_author], match)\n",
    "            if not match:\n",
    "                subID_author_to_votes[_subID_author], match, answer = code_match(AITA_codebook_expand, \n",
    "                                                row['comment'].lower(), subID_author_to_votes[_subID_author], match)\n",
    "\n",
    "        else:\n",
    "            subID_author_to_votes[_subID_author] = {'YTA':0, 'NTA':0, 'ESH':0, 'NAH':0, 'INFO':0}\n",
    "            match = False\n",
    "\n",
    "            subID_author_to_votes[_subID_author], match, answer = code_match(AITA_codebook, \n",
    "                                                row['comment'], subID_author_to_votes[_subID_author], match)\n",
    "            if not match:\n",
    "                subID_author_to_votes[_subID_author], match, answer = code_match(AITA_codebook_lower, \n",
    "                                                row['comment'], subID_author_to_votes[_subID_author], match)\n",
    "            if not match:\n",
    "                subID_author_to_votes[_subID_author], match, answer = code_match(AITA_codebook_expand, \n",
    "                                                row['comment'].lower(), subID_author_to_votes[_subID_author], match)\n",
    "\n",
    "        redditor_answers.append(answer)\n",
    "        if match:\n",
    "            matching_indices.append(idx)\n",
    "    \n",
    "    _df['judgment'] = redditor_answers\n",
    "    return _df[_df.index.isin(matching_indices)], subID_author_to_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original instances: 78525, situations: 59919, authors: 30\n",
      "matching instances: 69775, situations: 54661, authors: 30\n"
     ]
    }
   ],
   "source": [
    "df_Comms_AITA_top_agg_judge, subID_author_to_votes = add_judgment_remove_commsWithoutJudgment(df_Comms_AITA_top_agg)\n",
    "\n",
    "print('original instances: %d, situations: %d, authors: %d'%(len(df_Comms_AITA_top_agg), len(list(set(df_Comms_AITA_top_agg['id'].tolist()))), len(list(set(df_Comms_AITA_top_agg['author'].tolist())))))\n",
    "print('matching instances: %d, situations: %d, authors: %d'%(len(df_Comms_AITA_top_agg_judge), len(list(set(df_Comms_AITA_top_agg_judge['id'].tolist()))), len(list(set(df_Comms_AITA_top_agg_judge['author'].tolist())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original instances: 16394, situations: 10518, authors: 30\n",
      "matching instances: 14820, situations: 9929, authors: 30\n"
     ]
    }
   ],
   "source": [
    "df_topk_Comms_match, subID_author_to_votes = add_judgment_remove_commsWithoutJudgment(df_topk_Comms)\n",
    "\n",
    "print('original instances: %d, situations: %d, authors: %d'%(len(df_topk_Comms), len(list(set(df_topk_Comms['id'].tolist()))), len(list(set(df_topk_Comms['author'].tolist())))))\n",
    "print('matching instances: %d, situations: %d, authors: %d'%(len(matching_indices), len(list(set(df_topk_Comms_match['id'].tolist()))), len(list(set(df_topk_Comms_match['author'].tolist())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yh/opt/anaconda3/envs/dev37/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Add a binary 'label' column\n",
    "\n",
    "judgment_mapping = {'NAH':0, 'NTA':0, 'YTA':1, 'ESH':1, 'INFO':-1, '':-1}\n",
    "df_Comms_AITA_top_agg_judge['label'] = df_Comms_AITA_top_agg_judge['judgment'].map(lambda x:judgment_mapping[x])\n",
    "df_topk_Comms_match['label'] = df_topk_Comms_match['judgment'].map(lambda x:judgment_mapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add controversiality for future analysis\n",
    "def add_controv(_df, subID_author_to_votes):\n",
    "    subID_author_to_votesCntrov = {}\n",
    "\n",
    "    for key in subID_author_to_votes:\n",
    "        yes = subID_author_to_votes[key]['YTA'] + subID_author_to_votes[key]['ESH']\n",
    "        no = subID_author_to_votes[key]['NTA'] + subID_author_to_votes[key]['NAH']\n",
    "        try:\n",
    "            majority_percent = round(yes/(yes+no) if yes>no else no/(yes+no), 2)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        subID_author_to_votesCntrov[key] = majority_percent\n",
    "\n",
    "    for _id, _author in zip(_df['id'].tolist(), _df['author'].tolist()):\n",
    "        key = _id+'___###___'+_author\n",
    "        if key not in subID_author_to_votesCntrov:\n",
    "            subID_author_to_votesCntrov[key] = -1\n",
    "\n",
    "    _df['controv'] = _df.apply(lambda x:subID_author_to_votesCntrov[x['id']+'___###___'+x['author']], axis=1)\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topk_Comms_match = add_controv(df_topk_Comms_match, subID_author_to_votes)\n",
    "df_topk_Comms_match.to_csv(os.getcwd()+'/data/reddit-morality/CommsRoTs__(Sub_Sochem)__(Redditor_top)__(Judge_trunc).tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yh/opt/anaconda3/envs/dev37/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "df_Comms_AITA_top_agg_judge = add_controv(df_Comms_AITA_top_agg_judge, subID_author_to_votes)\n",
    "df_Comms_AITA_top_agg_judge.to_csv(os.getcwd()+'/data/reddit-morality/Comms__(Sub_AITA)__(Redditor_top)__agg__(Judge_trunc).tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
